

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Project Report &mdash; Machine Learning Compilers  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=f281be69"></script>
      <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="User Guide" href="03_user_guide.html" />
    <link rel="prev" title="Introduction" href="01_project_information.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Machine Learning Compilers
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Project Overview</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="01_project_information.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Project Report</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_docu_setup.html">Documentation Setup</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Submissions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../submissions/01_assembly.html">1. Assembly</a></li>
<li class="toctree-l1"><a class="reference internal" href="../submissions/02_base.html">2. Base</a></li>
<li class="toctree-l1"><a class="reference internal" href="../submissions/03_neon.html">3. Neon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../submissions/04_code_gen.html">4. Code Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../submissions/05_tensor_op.html">5. Tensor Operation Backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../submissions/06_einsum.html">6. Einsum Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../submissions/07_individual_phase.html">7. Individual Phase</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit.html">mini_jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_benchmarks.html">mini_jit::benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_converters.html">mini_jit::converters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_einsum.html">mini_jit::einsum</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_instructions.html">mini_jit::instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_ir.html">mini_jit::ir</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_kernels.html">mini_jit::kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/namespaces/mini_jit_registers.html">mini_jit::registers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Machine Learning Compilers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Project Report</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/project_overview/02_project_report.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="project-report">
<span id="id1"></span><h1>Project Report<a class="headerlink" href="#project-report" title="Link to this heading">ÔÉÅ</a></h1>
<p>In the first two weeks of our project, we focused on ARM AArch64 assembly to build a solid foundation for understanding machine learning compilers.
During the first week, we examined the assembly output of a simple C program compiled with both <strong>GCC</strong> and <strong>Clang</strong> to explore the differences in code generation and function call conventions.
This initial exploration helped us become familiar with compiler behavior, instruction-level representations, and low-level debugging tools such as <strong>GDB</strong>.
Further details about the specific steps and tasks can be found in the <a class="reference internal" href="../submissions/01_assembly.html#assembly"><span class="std std-ref">assembly section</span></a> of our project documentation.</p>
<p>In the second week, we began writing assembly programs from scratch using only base instructions.
Specifically, we reimplemented two simple C functions in AArch64 assembly to better understand data movements and control flow at the instruction level.
After successfully replicating the functionality of the C programs, we developed microbenchmarks to evaluate the <strong>throughput</strong> and <strong>latency</strong> of key instructions such as <code class="docutils literal notranslate"><span class="pre">ADD</span></code> and <code class="docutils literal notranslate"><span class="pre">MUL</span></code>.
These benchmarks helped us gain insight into the performance characteristics of modern ARM processors and how instruction-level behavior can impact overall computation speed.
Further details about the specific steps and tasks can be found in the <a class="reference internal" href="../submissions/02_base.html#base"><span class="std std-ref">base section</span></a> of our project documentation.</p>
<p>After spending the first two weeks experimenting with base instructions and writing simple assembly programs, we advanced to working with <strong>Neon</strong> (<strong>Advanced SIMD</strong>) instructions.
In the following weeks, we explored the performance characteristics of Neon operations, an essential step toward mastering the fundamentals required for building our own tensor compiler.</p>
<p>We began by benchmarking the <a class="reference internal" href="../submissions/03_neon.html#throughput-latency"><span class="std std-ref">throughput and latency</span></a> of the <code class="docutils literal notranslate"><span class="pre">FMLA</span></code> and <code class="docutils literal notranslate"><span class="pre">FMADD</span></code> instructions.
This helped us understand the significance of instruction-level parallelism and how much instruction ordering and data dependencies can impact performance.
After these initial experiments, we shifted our focus toward understanding the role of <strong>microkernels</strong>.
We explored key design considerations, like data reuse, register allocation, and memory access patterns, and conducted our first experiments with optimizing <a class="reference internal" href="../submissions/03_neon.html#microkernel"><span class="std std-ref">microkernels</span></a> for performance.</p>
<p>In the following week, we extended our microkernel by wrapping it in <strong>loops</strong> to support larger matrix dimensions and improve the performance.
Starting from our base kernel of <code class="docutils literal notranslate"><span class="pre">16x6x1</span></code>, we progressively scaled it to handle matrices of size <code class="docutils literal notranslate"><span class="pre">64x48x64</span></code>.
This allowed us to reach the architectural performance limits of a M4 Chip.
Further implementation details can be found in the <a class="reference internal" href="../submissions/03_neon.html#loops"><span class="std std-ref">loops section</span></a> of our project documentation.</p>
<p>After exploring ideal kernel sizes aligned with our vector register widths, we also experimented with cases where the <code class="docutils literal notranslate"><span class="pre">M</span></code> dimension is not a multiple of 4 or 16.
In these scenarios, special handling was required to maintain correctness and efficiency.
We implemented and optimized dedicated kernels for two such cases, which are documented in detail in the <a class="reference internal" href="../submissions/03_neon.html#simd"><span class="std std-ref">SIMD Lanes</span></a> section of our documentation.
In the same week, we also explored the impact of <strong>accumulator block</strong> shapes for performance reasons.
Specifically, we implemented a microkernel for computing <code class="docutils literal notranslate"><span class="pre">C+=AB</span></code> with dimensions <code class="docutils literal notranslate"><span class="pre">M=64</span></code>, <code class="docutils literal notranslate"><span class="pre">N=64</span></code>, and <code class="docutils literal notranslate"><span class="pre">K=64</span></code>.
This required adapting our existing <code class="docutils literal notranslate"><span class="pre">matmul_64_48_64</span></code> kernel to support the extended <code class="docutils literal notranslate"><span class="pre">N</span></code> dimension.
The details and benchmarking results of this extension are documented in the <a class="reference internal" href="../submissions/03_neon.html#accumulator-block-shapes"><span class="std std-ref">Accumulator Block Shapes</span></a> section.</p>
<p>After implementing and optimizing standard matrix multiplication kernels, we extended our work to support <a class="reference internal" href="../submissions/03_neon.html#batch-reduce-gemm"><span class="std std-ref">batch-reduce GEMM</span></a> operations.
We reused and adapted our existing microkernels to handle <strong>batches of matrices</strong> efficiently.</p>
<p>The last part of the Neon section was to explore how to <strong>transpose</strong> matrices using Neon assembly.
Our goal was to handle a <code class="docutils literal notranslate"><span class="pre">8x8</span></code> matrix, which we handled by dividing it into four <code class="docutils literal notranslate"><span class="pre">4x4</span></code> submatrices.
More details and code can be found in the <a class="reference internal" href="../submissions/03_neon.html#transposition"><span class="std std-ref">transposition section</span></a> of our documentation.</p>
<p>In week 4, we turned our attention to code generation.
The idea was to generate the previously implemented Neon assembly kernels using C++ during runtime.
Starting with a rather simple example, we began by implementing a <code class="docutils literal notranslate"><span class="pre">matmul_16_6_k</span></code> <a class="reference internal" href="../submissions/04_code_gen.html#brgemm-microkernel"><span class="std std-ref">microkernel</span></a>.
For this, we learned how to generate assembly instructions by setting each bit manually, writing the 32-bit instructions to previously allocated memory and lastly making that memory executable.</p>
<p>The next task was to extend the <code class="docutils literal notranslate"><span class="pre">matmul_16_6_k</span></code> by generating loops over the <code class="docutils literal notranslate"><span class="pre">M</span></code> and <code class="docutils literal notranslate"><span class="pre">N</span></code> dimensions, resulting in a <strong>GEMM</strong> kernel.
This also involved writing a backend entry point for kernel generation and unit tests for verification.
After verifying that the GEMM kernel generation worked as intended, we proceeded with implementing a <strong>Batch-Reduce GEMM (BRGEMM)</strong> kernel.
Here too, we verified our implementation using unit tests.
Lastly, we performed an extensive benchmark of the GEMM and BRGEMM kernels for different matrix dimensions.
In total, this benchmark took over 8 hours on the provided Apple M4 machine.
For more information, refer to <a class="reference internal" href="../submissions/04_code_gen.html#brgemm-primitive"><span class="std std-ref">4.1 BRGEMM Primitive</span></a>.</p>
<p>In week 5, we extended our code generator by unary primitives of the form B:=op(A).
Similarly to the BRGEMM backend, we first implemented a new entry point which can be used to generate various unary primitive kernels.
The first unary primitive we implemented was the <strong>Zero Primitive</strong>, which sets all elements of the output tensor to zero.
Secondly, we implemented the <strong>Identity Primitive</strong> which copies all elements of the input tensor to the output tensor.
The complicated part here was to support transposition for arbitrary tensor sizes.
Lastly, we implemented an activation function commonly found in machine learning frameworks: the <strong>ReLU Primitive</strong>.
This operation sets all negative elements to zero and keeps positive elements as they are.
For all implemented unary operations we implemented unit tests and benchmarked the performance.
Further information can be found in <a class="reference internal" href="../submissions/04_code_gen.html#unary-primitives"><span class="std std-ref">4.2 Unary Primitives</span></a>.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="01_project_information.html" class="btn btn-neutral float-left" title="Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="03_user_guide.html" class="btn btn-neutral float-right" title="User Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Lucas Obitz, Luca-Philipp Grumbach.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>